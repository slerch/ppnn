{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Set-up-data\" data-toc-modified-id=\"Set-up-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Set up data</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#RNNs-with-only-temperature-data\" data-toc-modified-id=\"RNNs-with-only-temperature-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>RNNs with only temperature data</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#RNN-predicting-only-the-last-target\" data-toc-modified-id=\"RNN-predicting-only-the-last-target-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>RNN predicting only the last target</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Sequence-RNN\" data-toc-modified-id=\"Sequence-RNN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sequence RNN</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Longer-sequence\" data-toc-modified-id=\"Longer-sequence-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Longer sequence</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Reference-experiment-with-longer-training-set.\" data-toc-modified-id=\"Reference-experiment-with-longer-training-set.-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Reference experiment with longer training set.</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Sequence-model-with-longer-training-set\" data-toc-modified-id=\"Sequence-model-with-longer-training-set-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Sequence model with longer training set</a></span></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Predict-only-one-value\" data-toc-modified-id=\"Predict-only-one-value-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predict only one value</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/rnn_test.ipynb#Get-additional-variables\" data-toc-modified-id=\"Get-additional-variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Get additional variables</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "In this notebook we will try out RNNs for our post-processing. The idea here is that there might be some extra information in looking at data from previous time steps.\n",
    "\n",
    "RNNs take quite a long time to train, so I am using a GPU here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda environment: py36_gpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from importlib import reload\n",
    "import crps_loss; reload(crps_loss)\n",
    "from crps_loss import crps_cost_function, crps_cost_function_seq\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \\\n",
    "    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this if you want to limit the GPU RAM usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "# DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "results_dir = '../results/'\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data\n",
    "\n",
    "This is now also done inside the `get_train_test_sets` function. `seq_len` is the number of timesteps (including the one to predict). We will start out with a moderate length of 5 days, training for 2015, predicting for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180849, 5, 2), (180849, 5, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape, train_set.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays have dimensions [sample, time step, feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs with only temperature data\n",
    "\n",
    "As a comparison. Our simple networks got a train/test loss of around 1.07/1.01.\n",
    "\n",
    "I am using a Gated Recurrent Unit (GRU) as my recurrent layer. LSTM is probably the more common one, but GRU is slightly cheaper and for our simple applications provides similar results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN predicting only the last target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "hidden_nodes = 100   # Number of hidden nodes inside RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes)(inp)\n",
    "x = Dense(2, activation='linear')(x)\n",
    "rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5, 2)              0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 31,102\n",
      "Trainable params: 31,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 4s - loss: 1.8331 - val_loss: 1.0214\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0469 - val_loss: 1.0294\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0440 - val_loss: 1.0365\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0351 - val_loss: 1.0207\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0329 - val_loss: 1.0377\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0325 - val_loss: 1.0131\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0248 - val_loss: 1.0213\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0248 - val_loss: 1.0329\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0213 - val_loss: 1.0208\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0156 - val_loss: 1.0418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc9eb437b8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(train_set.features, train_set.targets[:,-1], epochs=10, batch_size=batch_size,\n",
    "              validation_data=(test_set.features, test_set.targets[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a better train score and a worse validation score. This indicates overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 2)              0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 5, 100)            30900     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2)              202       \n",
      "=================================================================\n",
      "Total params: 31,102\n",
      "Trainable params: 31,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(model, train_set, test_set, epochs, batch_size):\n",
    "    \"\"\"Write our own function to train and validate, \n",
    "    because the keras fit function cannot handle sample weights for training\n",
    "    and validation at the same time.\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        print('Epoch:', i+1)\n",
    "        h = model.fit(train_set.features, train_set.targets, epochs=1, batch_size=batch_size, \n",
    "                      sample_weight=train_set.sample_weights, verbose=0)\n",
    "        print('Train:', h.history['loss'])\n",
    "        print('Valid', model.evaluate(test_set.features, test_set.targets, batch_size=4096, \n",
    "                       sample_weight=test_set.sample_weights, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 2.1824     \n",
      "Valid 1.02194318586\n",
      "Epoch: 1\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0548     \n",
      "Valid 1.01296814224\n",
      "Epoch: 2\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0478     \n",
      "Valid 1.01225032061\n",
      "Epoch: 3\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0446     \n",
      "Valid 1.01777627614\n",
      "Epoch: 4\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0441     \n",
      "Valid 1.01947451983\n",
      "Epoch: 5\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0421     \n",
      "Valid 1.0184237752\n",
      "Epoch: 6\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0391     \n",
      "Valid 1.01643501133\n",
      "Epoch: 7\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0382     \n",
      "Valid 1.02314476758\n",
      "Epoch: 8\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0373     \n",
      "Valid 1.01951744772\n",
      "Epoch: 9\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 3s - loss: 1.0358     \n",
      "Valid 1.01807210738\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with the first RNN above we seem to overfit to the dataset, but maybe not as strongly. Let's now try a more complex model with a longer sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 20, 2)             0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 20, 200)           121800    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 2)             402       \n",
      "=================================================================\n",
      "Total params: 122,202\n",
      "Trainable params: 122,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 18s - loss: 2.5563      ETA: 10s - lo - ETA: 9s - los - ETA: 8s - loss: 3 - ETA: 8s  - ETA: 6s - loss: 3.06 - ETA: 6s - ETA: 1s - loss: 2.702 - ETA: 1s - loss: 2.69 - ETA: 1s - loss: 2 - ETA: 1s - loss: 2.63 - ETA: 0s - loss: 2 - ETA: 0s - loss: 2.58 - ETA: 0s - loss: 2.56\n",
      "Valid 1.09413727563\n",
      "Epoch: 1\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0954    1 ETA: 16s - loss: 1. - ETA: 16s - loss - ETA: 16s - loss: 1. - ETA: 15s - loss: 1.26 - ETA: 15s - loss: 1.26 - ETA: 14s - lo - ETA: 13s - lo - ETA:  - ETA: 9 - ETA: 8s - loss: 1.1 - ETA - ETA: 6s - loss: 1.116 - ETA: 5s - los - ETA: 5s - l - ETA: 3s - loss - ETA: 3s - loss: - ETA:  - ETA: 0s - loss\n",
      "Valid 1.01743481942\n",
      "Epoch: 2\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0564    - ETA: 9s - loss: 1.06 - ETA: 9s - lo - ETA: 8s - loss: 1.0 - ETA: 7s - loss: 1 - ETA: 7s - l - ETA: 6s - loss: 1. - ETA: 5s -  - ETA: 2s - loss: 1.05 - ETA: - ETA: 0s - loss: 1.0\n",
      "Valid 1.04572273414\n",
      "Epoch: 3\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0489    8 ETA: 14s - loss:  - ETA: 14s - loss: 1. - ETA - ETA: 13s - loss:  - ETA: 12s - loss: 1. - ETA: 12s - loss: 1. - ETA: 9s  - ETA: 3s - los - ETA: 2s - loss - ETA: 1s  - ETA: 0s - loss: 1.0 - ETA: 0s - loss: 1.048\n",
      "Valid 1.0348635667\n",
      "Epoch: 4\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0419      ETA: - ETA: 6s - loss: 1.03 - ETA: 6s  - ETA: 4s - loss: 1.04 - ETA: 4s - loss: 1 - ETA: 4s - - ETA: 2s - loss: 1.0 - ETA:  - ETA: 0s - lo\n",
      "Valid 1.01168127525\n",
      "Epoch: 5\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0373    - ETA: 9s - loss: 1.0 - ETA: 9s - lo - ETA: 8s - loss: 1.03 - ETA: 8s - loss: 1. - ETA: 7s - loss: 1.03 - ET - ETA: 5s - loss: 1.0 - ETA:  - ETA: 3s - \n",
      "Valid 1.01859074728\n",
      "Epoch: 6\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0267      ETA:  - ETA: 12s - loss: 1. - ETA: 12s -  - ETA: 11 - ETA: 11s - loss: 1. - ETA: 10 - ETA: 10s - loss:  - ETA: 9s - loss - ETA: 8s - loss: 1. - ETA: 8s - loss:  - ETA: 7s - loss: 1 - ETA: - ETA: 5s - loss: 1.02 - ETA: 5s - - ETA: 4s - loss: 1.02 - E - ETA: 1s - lo - ETA: 0s - loss: 1.02 - ETA: 0s - loss:\n",
      "Valid 1.02702659271\n",
      "Epoch: 7\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0257    4 E - ETA: 15s - loss: 1.05 - ETA: 15s - loss: 1. - ETA: 13 - ETA:  - ETA:  - ETA: 9s -  - ETA: 8s - loss: 1.024 - ETA: 8s - loss - ETA: 7s - loss: 1.02 - ETA: 7s - loss: 1. - ETA: 7s - loss: 1.0 - ETA: 6s - loss: 1.02 - ETA: 6s - loss: 1. - ETA:  - ETA: 4s - loss: 1.02 - ETA: 4s - loss: 1.02 - ETA: 4s - loss: 1.02 - ETA: 3s - loss: 1.02 - ETA: 3s -\n",
      "Valid 1.01975782837\n",
      "Epoch: 8\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0159    - ETA: 7s - loss: - ETA: 7s - - ETA: 5s - loss: 1.01 - ETA: 5s - loss: 1.01 - ETA: 5s - loss: 1 - ETA: 4s - loss: 1.01 - ETA: 4s - loss: - ETA - ETA: 2s - lo - ETA: 1s - los - ETA: 0s - loss: 1.015 - ETA: 0s - loss: 1.015\n",
      "Valid 1.04456050902\n",
      "Epoch: 9\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 17s - loss: 1.0141      ETA: 14 - ETA: 13s - loss - ETA - ETA: 12s - loss:  - ETA:  - ETA: 1 - ETA: 2s  - ETA: 1s - loss: 1.01 - ETA: 1s - los - ETA: 0s - loss: 1.01\n",
      "Valid 1.07489963545\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again we are overfitting, but maybe there is something to be learned. Let's try a much longer training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference experiment with longer training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2008-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied from fc_network notebook\n",
    "def build_fc_model():\n",
    "    inp = Input(shape=(2,))\n",
    "    x = Dense(2, activation='linear')(inp)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model()\n",
    "fc_model.compile(optimizer=Adam(0.1), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1456977 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.3725 - val_loss: 1.0121\n",
      "Epoch 2/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0626 - val_loss: 1.0122\n",
      "Epoch 3/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0628 - val_loss: 1.0099\n",
      "Epoch 4/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0630 - val_loss: 1.0111\n",
      "Epoch 5/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0631 - val_loss: 1.0134\n",
      "Epoch 6/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0630 - val_loss: 1.0128\n",
      "Epoch 7/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0631 - val_loss: 1.0080\n",
      "Epoch 8/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0630 - val_loss: 1.0073\n",
      "Epoch 9/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0631 - val_loss: 1.0095\n",
      "Epoch 10/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.0630 - val_loss: 1.0088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc74391eb8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a small improvement. Now let's test our sequence model with a longer training period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence model with longer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "train_dates = ['2008-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)\n",
    "seq_rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train: [1.1365550085535305]\n",
      "Valid 0.998565860164\n",
      "Epoch: 2\n",
      "Train: [1.0129750551209091]\n",
      "Valid 1.02674336182\n",
      "Epoch: 3\n",
      "Train: [0.97901781529687926]\n",
      "Valid 1.03270412821\n",
      "Epoch: 4\n",
      "Train: [0.95060623054262205]\n",
      "Valid 1.03952660352\n",
      "Epoch: 5\n",
      "Train: [0.93341366950854698]\n",
      "Valid 1.04783327321\n"
     ]
    }
   ],
   "source": [
    "# This takes a while!\n",
    "train_and_valid(seq_rnn_model, train_set, test_set, 5, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After epoch one there is an improvement in the validation score, then we are starting to overfit again. So maybe some regularization is needed.\n",
    "\n",
    "I am not quite sure how to regularize GRUs properly. Using the parameter dropout gives me nans. Using recurrent_dropout does not, so let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True, recurrent_dropout=0.5)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)\n",
    "seq_rnn_model.compile(optimizer=Adam(0.001), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train: [1.6217677070568353]\n",
      "Valid 1.00813483157\n",
      "Epoch: 2\n",
      "Train: [1.0490734272999829]\n",
      "Valid 1.00000224211\n",
      "Epoch: 3\n",
      "Train: [1.0414916153958527]\n",
      "Valid 0.99807775358\n",
      "Epoch: 4\n",
      "Train: [1.0388923035127486]\n",
      "Valid 0.995825972757\n",
      "Epoch: 5\n",
      "Train: [1.0369893335706539]\n",
      "Valid 0.994749346544\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 5, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes)(inp)\n",
    "x = Dense(2, activation='linear')(x)\n",
    "rnn_model2 = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model2.compile(optimizer=Adam(0.001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 5, 2)              0         \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 20)                1380      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,422\n",
      "Trainable params: 1,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/5\n",
      "180849/180849 [==============================] - 3s - loss: 1.1698 - val_loss: 1.0789\n",
      "Epoch 2/5\n",
      "180849/180849 [==============================] - 3s - loss: 1.1198 - val_loss: 1.0523\n",
      "Epoch 3/5\n",
      "180849/180849 [==============================] - 3s - loss: 1.0891 - val_loss: 1.0390\n",
      "Epoch 4/5\n",
      "180849/180849 [==============================] - 3s - loss: 1.0695 - val_loss: 1.0285\n",
      "Epoch 5/5\n",
      "180849/180849 [==============================] - 3s - loss: 1.0560 - val_loss: 1.0261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb2d435e80>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model2.fit(x_seq_train, y_seq_train[:,-1], epochs=5, batch_size=1024,\n",
    "              validation_data=(x_seq_test, y_seq_test[:,-1]))\n",
    "#rnn_model2.fit(x_seq_train, y_seq_train[:,-1], epochs=10, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "aux_dict = OrderedDict()\n",
    "aux_dict['data_aux_geo_interpolated.nc'] = ['orog', \n",
    "                                            'station_alt', \n",
    "                                            'station_lat', \n",
    "                                            'station_lon']\n",
    "aux_dict['data_aux_pl500_interpolated_00UTC.nc'] = ['u_pl500_fc',\n",
    "                                                    'v_pl500_fc',\n",
    "                                                    'gh_pl500_fc']\n",
    "aux_dict['data_aux_pl850_interpolated_00UTC.nc'] = ['u_pl850_fc',\n",
    "                                                    'v_pl850_fc',\n",
    "                                                    'q_pl850_fc']\n",
    "aux_dict['data_aux_surface_interpolated_00UTC.nc'] = ['cape_fc',\n",
    "                                                      'sp_fc',\n",
    "                                                      'tcc_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=5, fill_value=-999., aux_dict=aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = train_set.features.shape[-1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, n_features, )) # time step, feature\n",
    "x = GRU(20, return_sequences=True)(inp)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "x = TimeDistributed(Dense(2))(x)\n",
    "rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.47494211695\n",
      "Valid 1.50654933895\n",
      "Test 0.977439547093\n",
      "Valid 0.976263109129\n",
      "Test 0.944071006104\n",
      "Valid 0.952683184602\n",
      "Test 0.927436171778\n",
      "Valid 0.947129741995\n",
      "Test 0.921549836288\n",
      "Valid 0.941121052018\n",
      "Test 0.910908860636\n",
      "Valid 0.946302759566\n",
      "Test 0.907522408847\n",
      "Valid 0.943646217648\n",
      "Test 0.899797803507\n",
      "Valid 0.943910612833\n",
      "Test 0.894670393442\n",
      "Valid 0.942409784089\n",
      "Test 0.891969689318\n",
      "Valid 0.9460832431\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rnn_model.fit(train_set.features, train_set.targets, epochs=1, batch_size=1024, \n",
    "                  sample_weight=train_set.sample_weights, verbose=0)\n",
    "    print('Test', rnn_model.evaluate(train_set.features, train_set.targets, batch_size=4096, \n",
    "                   sample_weight=train_set.sample_weights, verbose=0))\n",
    "    print('Valid', rnn_model.evaluate(test_set.features, test_set.targets, batch_size=4096, \n",
    "                   sample_weight=test_set.sample_weights, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, n_features, )) # time step, feature\n",
    "x = GRU(20)(inp)\n",
    "x = Dense(2, activation='linear')(x)\n",
    "rnn_model2 = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model2.compile(optimizer=Adam(0.01), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/5\n",
      "180849/180849 [==============================] - 3s - loss: 0.8975 - val_loss: 0.9574\n",
      "Epoch 2/5\n",
      "180849/180849 [==============================] - 3s - loss: 0.8868 - val_loss: 0.9616\n",
      "Epoch 3/5\n",
      "180849/180849 [==============================] - 3s - loss: 0.8791 - val_loss: 0.9473\n",
      "Epoch 4/5\n",
      "180849/180849 [==============================] - 3s - loss: 0.8746 - val_loss: 0.9792\n",
      "Epoch 5/5\n",
      "180849/180849 [==============================] - 3s - loss: 0.8690 - val_loss: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb07e897b8>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model2.fit(train_set.features, train_set.targets[:,-1], epochs=5, batch_size=1024,\n",
    "              validation_data=(test_set.features, test_set.targets[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
